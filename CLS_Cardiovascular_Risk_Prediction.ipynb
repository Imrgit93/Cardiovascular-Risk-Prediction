{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "Yfr_Vlr8HBkt",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imrgit93/Cardiovascular-Risk-Prediction/blob/main/CLS_Cardiovascular_Risk_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Cardiovascular Risk Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is from an ongoing cardiovascular study on residents of the town of Framingham,\n",
        "Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of\n",
        "future coronary heart disease (CHD). The dataset provides the patients’ information. It includes\n",
        "over 4,000 records and 15 attributes.\n",
        "Variables\n",
        "Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk\n",
        "factors.\n",
        "\n",
        "Data Description\n",
        "\n",
        "Demographic:\n",
        "\n",
        "• Sex: male or female(\"M\" or \"F\")\n",
        "\n",
        "• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to\n",
        "whole numbers, the concept of age is continuous)\n",
        "Behavioral\n",
        "\n",
        "• is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "\n",
        "• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be\n",
        "considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "Medical( history)\n",
        "\n",
        "• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
        "\n",
        "• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
        "\n",
        "• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
        "\n",
        "• Diabetes: whether or not the patient had diabetes (Nominal)\n",
        "Medical(current)\n",
        "\n",
        "• Tot Chol: total cholesterol level (Continuous)\n",
        "\n",
        "• Sys BP: systolic blood pressure (Continuous)\n",
        "\n",
        "• Dia BP: diastolic blood pressure (Continuous)\n",
        "\n",
        "• BMI: Body Mass Index (Continuous)\n",
        "\n",
        "• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in\n",
        "fact discrete, yet are considered continuous because of large number of possible values.)\n",
        "\n",
        "• Glucose: glucose level (Continuous)\n",
        "Predict variable (desired target)\n",
        "\n",
        "• 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”) -\n",
        "Dependent Variable"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Imrgit93/Cardiovascular-Risk-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Problem Statement Here.**\n",
        "\n",
        "Despite of the age, rise in number of Cardiovascular Heart Disease (CHD) is the one of the leading reason for death annually worldwide in recent past years. However,it can be privented if caught early and simple changes are made in lifestyle . This project would explore a set of given data and known factors for heart disease to develop a classification machine learning model to predict risk of developing heart disease within the next ten years."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the file\n",
        "CR_df = pd.read_csv('/content/drive/MyDrive/Alma Better/Capstone _Project/3 - CLS_Cardiovascular Risk Prediction/data_cardiovascular_risk .csv')"
      ],
      "metadata": {
        "id": "uLtrtINvwkDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "CR_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "CR_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "CR_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "CR_df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO Duplicate values in this data set"
      ],
      "metadata": {
        "id": "QNQ54n_Szqs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "CR_df.isna().sum().sort_values(ascending=False)[:10].reset_index().rename(columns={'index':'columns',0:'nullvalues'})"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer :\n",
        "\n",
        "Data set have 3390 rows and 17columns.\n",
        "\n",
        "There no duplicate values.\n",
        "\n",
        "There are null values :-\n",
        "\n",
        "1 glucose\t304\n",
        "\n",
        "2\teducation\t87\n",
        "\n",
        "3\tBPMeds\t44\n",
        "\n",
        "4\ttotChol\t38\n",
        "\n",
        "5\tcigsPerDay\t22\n",
        "\n",
        "6\tBMI\t14\n",
        "\n",
        "7\theartRate\t1\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "CR_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "CR_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demographic:**\n",
        "\n",
        "• Sex: male or female(\"M\" or \"F\")\n",
        "\n",
        "• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "\n",
        "**Behavioral**\n",
        "\n",
        "• is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "\n",
        "• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be\n",
        "considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "\n",
        "**Medical( history)**\n",
        "\n",
        "• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
        "\n",
        "• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
        "\n",
        "• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
        "\n",
        "• Diabetes: whether or not the patient had diabetes (Nominal)\n",
        "Medical(current)\n",
        "\n",
        "• Tot Chol: total cholesterol level (Continuous)\n",
        "\n",
        "• Sys BP: systolic blood pressure (Continuous)\n",
        "\n",
        "• Dia BP: diastolic blood pressure (Continuous)\n",
        "\n",
        "• BMI: Body Mass Index (Continuous)\n",
        "\n",
        "• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in\n",
        "fact discrete, yet are considered continuous because of large number of possible values.)\n",
        "\n",
        "• Glucose: glucose level (Continuous)\n",
        "\n",
        "**Predict variable (desired target)**\n",
        "\n",
        "• 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”) -\n",
        "DV"
      ],
      "metadata": {
        "id": "36USwyxvI9WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in CR_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",CR_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Feature Engineering***"
      ],
      "metadata": {
        "id": "4-tE0tWWJObh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new copy\n",
        "CR_df1 = CR_df.copy()"
      ],
      "metadata": {
        "id": "3q0b8xpZQ_9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of columns with null values\n",
        "Mising_columns = ['glucose','education','BPMeds','totChol','cigsPerDay','BMI','heartRate']"
      ],
      "metadata": {
        "id": "tlQY7d1sIZEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot for columns having null values\n",
        "for i in Mising_columns:\n",
        "  plt.subplots(figsize=(10,5))\n",
        "  sns.distplot(CR_df[i],kde = False)\n",
        "  plt.ylabel('count')"
      ],
      "metadata": {
        "id": "qTbRbboxJodS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "Since glucose have 304 missing valuw and data is right Skewed we will replace it with median value\n",
        "\n",
        "As column education have  87 missing value ,will conside replace null value as 0(others) \n",
        "\n",
        "As column BPMeds 44 missing value replace null value as 2(others) 0 is already used.\n",
        "\n",
        "column totChol have 38 missing value and data is right Skewed we will replace it with median value.\n",
        "\n",
        "column cigsPerDay have 22 missing value replace null value as 0 mode.\n",
        "\n",
        "column BMI have have 14 missing value and data is right Skewed we will replace it with median value\n",
        "\n",
        "column heartRate have 1 missing value even if data is not exactly a normal distribution replace it with median.\n",
        "\n"
      ],
      "metadata": {
        "id": "bYXvD3iOLKXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "HwY0_2LsJObi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing nan values as explained above\n",
        "#column Glucose\n",
        "CR_df1.glucose.fillna(CR_df1.glucose.median(),inplace=True ) \n",
        "#column education\n",
        "CR_df1.education.fillna(0, inplace=True) \n",
        "#column BPMeds\n",
        "CR_df1.BPMeds.fillna(2, inplace=True)\n",
        "#column totchol\n",
        "CR_df1.totChol.fillna(CR_df1.totChol.median(), inplace=True)\n",
        "#column cigs\n",
        "CR_df1.cigsPerDay.fillna(0, inplace=True) \n",
        "#column BMI\n",
        "CR_df1.BMI.fillna(CR_df1.BMI.median(),inplace=True ) \n",
        "#column heart Rate\n",
        "CR_df1.heartRate.fillna(CR_df1.heartRate.median(),inplace=True )"
      ],
      "metadata": {
        "id": "mnMjZ2nbJi5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Categorical Encoding"
      ],
      "metadata": {
        "id": "qUHR_Q3jJObj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy data \n",
        "CR_df2 = CR_df1.copy()"
      ],
      "metadata": {
        "id": "flUhEP3lponJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert categorical data to Numeric\n",
        "CR_df2['sex'] = CR_df2.sex.map(dict(M = 0, F = 1)) \n",
        "CR_df2['is_smoking'] = CR_df2.is_smoking.map(dict(YES = 1, NO = 0))"
      ],
      "metadata": {
        "id": "go63Vym4Lm2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross checking for null values\n",
        "CR_df2.isna().sum().sort_values(ascending=False)[:5].reset_index().rename(columns={'index':'columns',0:'nullvalues'})"
      ],
      "metadata": {
        "id": "P3z-EVjPSxYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CR_df2.info()"
      ],
      "metadata": {
        "id": "aXad6pUXjxXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Age_df = CR_df2.copy()"
      ],
      "metadata": {
        "id": "L4mn60HM_4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new column to find smoing count for both category people of smoking and not smoking\n",
        "Age_df['smoking_count'] = Age_df['is_smoking']"
      ],
      "metadata": {
        "id": "UZNt2A6j_-J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Age_df1 = Age_df.groupby(['age','is_smoking']).agg({'smoking_count':'count','cigsPerDay':'mean','diabetes':'sum','totChol':'mean','heartRate':'mean'}).sort_values(by=['age']).reset_index()\n",
        "Age_df1"
      ],
      "metadata": {
        "id": "WMYcV9Xuc1jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was really no need for data wangling ,however very few are done during data vizualization "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#Univarinet analysis for target variable\n",
        "CR_df2['TenYearCHD'].value_counts()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='TenYearCHD',data=CR_df2)"
      ],
      "metadata": {
        "id": "XvjDAwfZOdYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)\n",
        "\n",
        "Well,people with  no heart disease are more ,but as this is our target variable it leads to imbalanec data set .Therefore further data imbaance manupulation will be done befero using this data in model deploement"
      ],
      "metadata": {
        "id": "WbwO32fpOwgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#List of idependent variable\n",
        "column_uva = CR_df2.columns[1:-1]\n",
        "column_uva"
      ],
      "metadata": {
        "id": "UmLmgI_SP6aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#value count for all the indidpendent variables.\n",
        "for i in column_uva:\n",
        "  In_variable = CR_df2[i].value_counts()\n",
        "  print( f'for column {i}' )\n",
        "  print(In_variable)\n",
        "  print('-----'*20)"
      ],
      "metadata": {
        "id": "IxzeVhSOD-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot for Univarient Analsis\n",
        "for i in column_uva:\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.distplot(CR_df2[i],kde = False)\n",
        "    plt.xlabel(i)\n",
        "    plt.ylabel('count')"
      ],
      "metadata": {
        "id": "3lj5hBlUIM8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "Sample was taken in age group of people between 32 to 70\n",
        "\n",
        "Education Level: 1 with count of 1391, 2 with count of 990, 3 with count of 549, 4 with count of 373,0.0 with count of  87\n",
        "\n",
        "Gender, 0=Male with 1467 count , 1 = Female with 1923 count\n",
        "\n",
        "patient is a  smoker 1= yes with count 1687 , 0=no with count  1703 almost balanced set\n",
        "\n",
        "Number of Cigarettes smoked per day value range 0 to 70  high is as :\n",
        "\n",
        "0.0    --   1725\n",
        "\n",
        "20.0   --    606\n",
        "\n",
        "30.0    --   176\n",
        "\n",
        "15.0     --  172\n",
        "\n",
        "Blood Pressure Medications 0 = no with count of 3246, 1=yes with count of 100, 2 = unknown with count of 44\n",
        "\n",
        "Prevalence of stroke 0=none with count of 3368, 1 = had occurences of stroke with count of 22 \n",
        "\n",
        "Prevalence of hypertension 0=none with count of 2321 , 1= has prevalence hypertension with count of 1069\n",
        "\n",
        "patient has diabetes 0=no with count of 3303 , 1=yes with count of  87\n",
        "\n",
        "Total Cholesterol value range between 107-696 where as 125 to 200 is optimum range our data set is highly concentrated between 200 to 270 is right skewed\n",
        "\n",
        "systolic blood pressure value range between 83 - 295 where as normal 120-130 and our data is between 100 to 150 is nearly right skewed\n",
        "\n",
        "diastolic blood pressure A normal range for adults is 60 mmHg to 80 most of  our data is between 70 to 100 is near to normally distributed.\n",
        "\n",
        "Body Mass Index most of  our data is between 20 to 30 is nearly right skewed.\n",
        "\n",
        "Heart Rate a normal range 70 to 110 bpm  most of  our data is between 60 to 100 is near to normally distributed.\n",
        "\n",
        " Glucose (72 to 99 mg/dL) when fasting  (140 mg/dL) 2 hours after eating most of  our data is between 60 to 90 and right skewed\n"
      ],
      "metadata": {
        "id": "_z6j4ZU_GKpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bivarient analysis on target variable to each independent varaible.\n",
        "#plot size.\n",
        "fig, ax = plt.subplots(5,3, figsize=(24, 12))\n",
        "\n",
        "for i in range(len(list(column_uva))):\n",
        "    plt.subplot(3,5,i+1)\n",
        "    col = list(column_uva)[i]\n",
        "    sns.kdeplot(CR_df2.loc[(CR_df2['TenYearCHD']==0),col], shade=True, label='No Risk', color='b')\n",
        "    sns.kdeplot(CR_df2.loc[(CR_df2['TenYearCHD']==1),col], shade=True, label='CHD Risk', color='r')\n",
        "    #plt.title(col)"
      ],
      "metadata": {
        "id": "7IpGWc3feTUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "As age of people Increases CHD risk increased\n",
        "\n",
        "Education Level: of every group had both almost same proportion between having and not having risk however no risk is greater.\n",
        "\n",
        "Gender, 0=Male had high chances of CHD Risk , 1 = Female had more cout on no risk.\n",
        "\n",
        "patient is a smoker 1= yes, 0=no have same proportion to having and not having risk\n",
        "\n",
        "Number of Cigarettes smoked per day if increased definatly increaded the CHD risk\n",
        "\n",
        "Blood Pressure Medications 0 = no ,1=yes has more CHD Risk  \n",
        "\n",
        "\n",
        "Prevalence of hypertension 0=none with more No Risk , 1= has prevalence hypertension had equal share of CHD risk\n",
        "\n",
        "patient has diabetes 0=no with more No Risk, 1=yes had equal share of CHD risk\n",
        "\n",
        "Every range of total Cholesterol have both NO Risk and CHD risk.\n",
        "\n",
        "systolic blood pressure has CHD risk if increased.\n",
        "\n",
        "diastolic blood pressure has CHD risk if increased.\n",
        "\n",
        "Body Mass Index has CHD risk if increased.\n",
        "\n",
        "Heart Rate has almost equal distribution of NO Risk and CHD risk\n",
        "\n",
        "Glucose has CHD risk if increased"
      ],
      "metadata": {
        "id": "8zCOchEggxIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bp check on both the genders.\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(1,2,1)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==0),'sysBP'], shade=True, color='b', Label='Male',legend=True)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==1),'sysBP'], shade=True, color='r', Label='Female',legend=True)\n",
        "plt.title('Men VS women to sysBP')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==0),'diaBP'], shade=True, Label='Male', color='b',legend=True)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==1),'diaBP'], shade=True, Label='Female', color='r',legend=True)\n",
        "plt.title('Men VS women to diaBP')"
      ],
      "metadata": {
        "id": "idElLfsk33e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "Man and Women had same distribution over range of systolic blood pressure and diastolic blood pressure"
      ],
      "metadata": {
        "id": "11UA4lf4CtCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check on total colestrol and and heartrate on gender.\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(1,2,1)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==0),'totChol'], shade=True, color='b', Label='Male',legend=True)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==1),'totChol'], shade=True, color='r', Label='Female',legend=True)\n",
        "plt.title('Men VS women to totChol')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==0),'heartRate'], shade=True, Label='Male', color='b',legend=True)\n",
        "sns.kdeplot(CR_df2.loc[(CR_df2['sex']==1),'heartRate'], shade=True, Label='Female', color='r',legend=True)\n",
        "plt.title('Men VS women to heartRate')  "
      ],
      "metadata": {
        "id": "cQEvy7SeY7Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "Women were bit  more in both total colestrol and heartrate."
      ],
      "metadata": {
        "id": "-Z9B8qwccu99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "8sRAYSRfDPbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#List of columns for multivarient analysis with age and target variable.\n",
        "age_mul_col = ['totChol', 'sysBP',\n",
        "       'diaBP', 'BMI', 'heartRate', 'glucose']"
      ],
      "metadata": {
        "id": "wXiQe9Ugi2VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(3,2, figsize=(24, 12))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
        "\n",
        "for i in range(len(age_mul_col)):\n",
        "    plt.subplot(3,2,i+1)\n",
        "    col = age_mul_col[i]\n",
        "    sns.scatterplot(x='age',y= col, data=CR_df2,palette='bright',hue='TenYearCHD')\n",
        "    plt.title(f'age VS {col} to TenYearCHD')\n"
      ],
      "metadata": {
        "id": "Wp0Bfvx1jHb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "Different age group had almost  same share of  totChol,BMI,heart rate ,sysBP and diaBP but as age is incerased CHD risk increased\n",
        "\n",
        "As age increases count for glucose increased which also had hige Risk in CHD\n",
        "\n"
      ],
      "metadata": {
        "id": "9lJiZVa4Hxsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#multivarient analysis with cigsPerDay and target variable.\n",
        "fig, ax = plt.subplots(3,2, figsize=(24, 12))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
        "\n",
        "for i in range(len(age_mul_col)):\n",
        "    plt.subplot(3,2,i+1)\n",
        "    col = age_mul_col[i]\n",
        "    sns.scatterplot(x='cigsPerDay',y= col, data=CR_df2,palette='bright',hue='TenYearCHD')\n",
        "    plt.title(f'cigsPerDay VS {col} to TenYearCHD')"
      ],
      "metadata": {
        "id": "AAoQ8CMvouxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "It was not a linear relationship , 3 varaible had same impact in all the plots\n"
      ],
      "metadata": {
        "id": "yOplEl41KsmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,12)) \n",
        "sns.heatmap(CR_df2.corr()[1:], annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "rB75jZcMrhks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "1)age has strongest weight in predicting CHD  , followed by blood pressure variable,glucose,diastolic ,Prevalence of hypertension\n",
        "\n",
        "2)  Blood glucose and presence of diabetes are closely related, also systolic and diastolic blood pressure ,and is_smoking with cigerday  which  was expected. \n",
        "\n",
        "3)Prevalence of hypertension has strong relation with systolic blood pressure and diastolic blood pressure.\n"
      ],
      "metadata": {
        "id": "CezukSioLQj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define dataframes to compare means between each outcome\n",
        "#binary: “1”, means “Yes”, “0” means “No” CHD\n",
        "No_CHD = CR_df2[CR_df2.TenYearCHD == 0]\n",
        "has_CHD = CR_df2[CR_df2.TenYearCHD == 1]"
      ],
      "metadata": {
        "id": "RYBrToWGOADz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to find mean of age in both having and not having CHD risk.\n",
        "sns.displot(CR_df2.age,color='blue',kde = False)\n",
        "plt.axvline(No_CHD.age.mean(),color='blue',label = 'no Risk')\n",
        "plt.axvline(has_CHD.age.mean(),color='red',label = 'Risk')"
      ],
      "metadata": {
        "id": "CJDDl_RJTZuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : Increasing age donot have the same risk of developing heart disease.\n",
        "\n",
        "Alternate Hypothesise : Increasing age have the  risk of developing heart disease."
      ],
      "metadata": {
        "id": "AlRcxeRaNfjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1, p1 = stats.ttest_ind(No_CHD.age, has_CHD.age, equal_var=False)\n",
        "print(f'ttest_ind:            t ={t1}  p = {p1}')\n",
        "if p1 < 0.05 :\n",
        "  print('Reject the null hypothesis.Increasing age have the risk of developing heart disease' )   \n",
        "else:\n",
        "   print('Fail to reject the null hypothesis.Increasing age donot have the same risk of developing heart disease')\n",
        "       "
      ],
      "metadata": {
        "id": "bCCHExC9PNfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-testing is used to compare the means between groups of continuous data"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to find mean of age in both having and not having CHD risk.\n",
        "sns.displot(CR_df2.cigsPerDay,color='blue',kde = False)\n",
        "plt.axvline(No_CHD.cigsPerDay.mean(),color='blue',label = 'no Risk')\n",
        "plt.axvline(has_CHD.cigsPerDay.mean(),color='red',label = 'Risk')"
      ],
      "metadata": {
        "id": "rE-n2v_9cHyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : CigsPerDay donot have the same risk of developing heart disease.\n",
        "\n",
        "Alternate Hypothesise : CigsPerDay have the  risk of developing heart disease."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t1, p1 = stats.ttest_ind(No_CHD.cigsPerDay, has_CHD.cigsPerDay, equal_var=False)\n",
        "print(f'ttest_ind:            t ={t1}  p = {p1}')\n",
        "if p1 < 0.05 :\n",
        "  print('Reject the null hypothesis.cigsPerDay have the risk of developing heart disease' )   \n",
        "else:\n",
        "   print('Fail to reject the null hypothesis.cigsPerDay donot have the same risk of developing heart disease')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-testing is used to compare the means between groups of continuous data"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data = CR_df2,x = 'sex' ,y ='TenYearCHD' )"
      ],
      "metadata": {
        "id": "4LWHL8oPBOjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : There is no relatiionship between gender and CHD risk\n",
        "\n",
        "Alternate Hypothesise : There is a relatiionship between gender and CHD risk."
      ],
      "metadata": {
        "id": "axHPGLPhDnlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency"
      ],
      "metadata": {
        "id": "tfO6XA4T9WOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "s_chd_td = pd.crosstab(index = CR_df2.sex,columns = CR_df2.TenYearCHD)\n",
        "chi2, pval , dof, expctd  = chi2_contingency(s_chd_td )\n",
        "print(f'Chi-Squared Value: {chi2}')\n",
        "print(f'P-value:    {pval} ')\n",
        "if pval < 0.05 :\n",
        "      print('Reject the null hypothesis,and thereis a relatiionship between gender and CHD risk' )\n",
        "else:\n",
        "      print('Fail to reject the null hypothesis,and there is no relatiionship between gender and CHD risk')\n",
        "        "
      ],
      "metadata": {
        "id": "swZR0D8r70kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Squared testing is used  when testing statistical independence or associaton between categorical variables"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6.Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "CR_df2.columns"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [round(variance_inflation_factor(X.values, i),2) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "5qYhzETHZef1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(CR_df2[[i for i in CR_df2.describe().columns if i not in ['TenYearCHD']]])"
      ],
      "metadata": {
        "id": "HRFApBx8Z0cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(CR_df2[[i for i in CR_df2.describe().columns if i not in ['TenYearCHD','id','totChol','diaBP','glucose','BMI','heartRate','sysBP']]])"
      ],
      "metadata": {
        "id": "-E6fhJtSaLHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_column = CR_df2[[i for i in CR_df2.describe().columns if i not in ['TenYearCHD','id','totChol','diaBP','glucose','BMI','heartRate','sysBP']]]"
      ],
      "metadata": {
        "id": "gPd32Tf_c6PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Final_column"
      ],
      "metadata": {
        "id": "iW2rZ29XdPun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used the VIF to select our feature ,to avoid model being biase."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for data balance\n",
        "CR_df2['TenYearCHD'].value_counts().plot.pie(explode=[0.05, 0.05], autopct='%1.1f%%', figsize=(10,8),fontsize=16)\n",
        "plt.title('TenYearCHD')"
      ],
      "metadata": {
        "id": "-8ANtfLE2aaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Before smote, counts of label '1': {}\".format(sum(CR_df2['TenYearCHD'] == 1)))\n",
        "print(\"Before smote, counts of label '0': {} \\n\".format(sum(CR_df2['TenYearCHD'] == 0)))"
      ],
      "metadata": {
        "id": "RSx4j-926Bma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,our data is highly imbalanced as 0 has 85% and 1 had only 15% "
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset\n",
        "# pip install imblearn (if you don't have imblearn in your system)\n",
        "!pip install imblearn"
      ],
      "metadata": {
        "id": "AeyJZOsz7Uyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import SMOTE module from imblearn library\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 42)\n",
        "X_sm, y_sm = sm.fit_resample(Final_column, CR_df2['TenYearCHD'])"
      ],
      "metadata": {
        "id": "Jij1QHAf3NU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After smote, counts of label '1': {}\".format(sum( y_sm == 1)))\n",
        "print(\"After smote, counts of label '0': {} \\n\".format(sum( y_sm == 0)))"
      ],
      "metadata": {
        "id": "2JIwOsAV6hAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_sm.value_counts().plot.pie(explode=[0.05, 0.05], autopct='%1.1f%%', figsize=(10,8),fontsize=16)\n",
        "plt.title('TenYearCHD')"
      ],
      "metadata": {
        "id": "Z1d6_tqg5JXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) for balanced  dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_sm)"
      ],
      "metadata": {
        "id": "BAFFDSmXem-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X\n"
      ],
      "metadata": {
        "id": "1NsWkNrPe11E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train_sm, X_test, y_train_sm, y_test = train_test_split(X,y_sm, test_size = 0.2, random_state = 11)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=100)\n",
        "clf.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "IlBPyPPHCrP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model and  predicted probabilities\n",
        "lg_train_preds = clf.predict_proba(X_train_sm)\n",
        "lg_test_preds = clf.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "kH9Ac5ZiDbLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get  predicted classes\n",
        "lg_train_class_preds = clf.predict(X_train_sm)\n",
        "lg_test_class_preds = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "vw7H_l3cFc05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy = accuracy_score(lg_train_class_preds,y_train_sm)\n",
        "test_accuracy = accuracy_score(lg_test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy)\n",
        "print(\"The accuracy on test data is \", test_accuracy)"
      ],
      "metadata": {
        "id": "S7r8EKtyGjz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = [ 'No Risk','Risk']\n",
        "lg_cm = confusion_matrix(y_test,lg_test_class_preds)\n",
        "print(lg_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(lg_cm, annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, lg_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "X8N6Ka-ypyIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "solvers = ['lbfgs']\n",
        "penalty = ['l2']\n",
        "c_values = [1000,100, 10, 1.0, 0.1, 0.01,0.001]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "lr_grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_grid_result=lr_grid_search.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "print(\"Best: %f using %s\" % (lr_grid_result.best_score_, lr_grid_result.best_params_))\n",
        "print(f'Best:estimator is {lr_grid_result.best_estimator_}')\n"
      ],
      "metadata": {
        "id": "m8W5fVBttrs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_optimal_model = lr_grid_result.best_estimator_"
      ],
      "metadata": {
        "id": "GYYeDXaYvpgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted classes\n",
        "lr_opt_train_class_preds = lr_grid_result.predict(X_train_sm)\n",
        "lr_opt_test_class_preds = lr_grid_result.predict(X_test)"
      ],
      "metadata": {
        "id": "ajTdweW3ubMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "lr_opt_train_accuracy = accuracy_score(lr_opt_train_class_preds,y_train_sm)\n",
        "lr_opt_test_accuracy = accuracy_score(lr_opt_test_class_preds,y_test)"
      ],
      "metadata": {
        "id": "YqlOudNHaKk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The accuracy on train data is \", lr_opt_train_accuracy)\n",
        "print(\"The accuracy on test data is \", lr_opt_test_accuracy)"
      ],
      "metadata": {
        "id": "Z3lykT7caeLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = [ 'No Risk','Risk']\n",
        "lr_opt_cm = confusion_matrix(y_test, lr_opt_test_class_preds)\n",
        "print(lr_opt_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(lr_opt_cm , annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "pgzmzenwapoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, lr_opt_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "_6xqYN6KKv4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.But,this makes the processing time-consuming and expensive based on the number of hyperparameters involved"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For  dataset before tuning,\n",
        "\n",
        "The accuracy on train data is  0.667390360399479\n",
        "\n",
        "The accuracy on test data is  0.6996527777777778\n",
        "\n",
        "**Confusion matrix:**\n",
        "\n",
        "                precision    recall  f1-score   \n",
        "           0      0.719     0.702     0.710      \n",
        "           1      0.680     0.697     0.688 \n",
        "\n",
        "For  dataset After tuning,\n",
        "\n",
        "The accuracy on train data is  0.6682587928788537\n",
        "\n",
        "The accuracy on test data is  0.6961805555555556\n",
        "\n",
        "**Confusion matrix:**\n",
        "\n",
        "                 precision    recall  f1-score   \n",
        "           0      0.722     0.684     0.702       \n",
        "           1      0.671     0.710     0.690       \n",
        "\n",
        "\n",
        "Not a big diference is seen here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "hMQndSm0HOlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "2Jp2IzwPJOs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = rf_model.get_params()"
      ],
      "metadata": {
        "id": "DpNiGRxNSY6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "rf_train_class_preds = rf_model.predict(X_train_sm)\n",
        "rf_test_class_preds = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "d3D6VuycJgAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "rf_train_accuracy = accuracy_score(y_train_sm,rf_train_class_preds)\n",
        "rf_test_accuracy = accuracy_score(y_test,rf_test_class_preds)\n",
        "\n",
        "print(\"The accuracy on train dataset is\", rf_train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", rf_test_accuracy)"
      ],
      "metadata": {
        "id": "2abOhspHJ4_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = [ 'No Risk','Risk']\n",
        "rf_cm = confusion_matrix(y_test,rf_test_class_preds)\n",
        "print(rf_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(rf_cm, annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, rf_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "hQInoaLpLxGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "'''\n",
        "n_estimators = [40,50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8,10]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [30,40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "              '''"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "n_estimators = [80]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [5,8,10]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [20,30,50]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [10,20,30]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "              '''"
      ],
      "metadata": {
        "id": "l9D-WJE3FEvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators = [80]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [20]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [20]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [4]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "Oyfvtzr5I1JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = cv, verbose=2, scoring='roc_auc')"
      ],
      "metadata": {
        "id": "PA9e_gPyhOXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(max_depth=10, min_samples_leaf=30, min_samples_split=50)\n",
        "#(max_depth=10, min_samples_leaf=10, min_samples_split=20,n_estimators=80)\n",
        "rf_grid.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "wmh8rRfxh7AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_grid.best_estimator_"
      ],
      "metadata": {
        "id": "zMDNsUhEiOWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_optimal_model = rf_grid.best_estimator_"
      ],
      "metadata": {
        "id": "2vv7QC8kiSjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_optimal_model.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "YWabBAUJGGrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "rf_opt_train_class_preds = rf_optimal_model.predict(X_train_sm)\n",
        "rf_opt_test_class_preds = rf_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "8AGTLIzKGU8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "rf_opt_train_accuracy = accuracy_score(y_train_sm,rf_opt_train_class_preds)\n",
        "rf_opt_test_accuracy = accuracy_score(y_test,rf_opt_test_class_preds)\n",
        "print(\"The accuracy on train dataset is\", rf_opt_train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", rf_opt_test_accuracy)"
      ],
      "metadata": {
        "id": "3PWBFuFWGhaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [ 'No Risk','Risk']\n",
        "rf_opt_cm = confusion_matrix(y_test,rf_opt_test_class_preds)\n",
        "print(rf_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(rf_opt_cm, annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "-YJLqTLPKa-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test,rf_opt_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "oxtAxH8GMNQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for data set before tuning:\n",
        "\n",
        "The accuracy on train dataset is 0.9122883195831524\n",
        "\n",
        "The accuracy on test dataset is 0.8376736111111112\n",
        "\n",
        "**confusion Matrix**\n",
        "\n",
        "                precision    recall  f1-score  \n",
        "           0      0.854     0.833     0.843       \n",
        "           1      0.821     0.843     0.832  \n",
        "\n",
        " for data setAfter tuning:\n",
        "\n",
        "The accuracy on train dataset is 0.8024316109422492\n",
        "The accuracy on test dataset is 0.7838541666666666 \n",
        "\n",
        "**confusion Matrix**\n",
        "\n",
        "               precision    recall  f1-score   \n",
        "           0      0.799     0.785     0.792       \n",
        "           1      0.767     0.783     0.775       \n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()"
      ],
      "metadata": {
        "id": "V3_MygswKrNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "i7o-IYrAKuXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "xg_train_class_preds = xg_models.predict(X_train_sm)\n",
        "xg_test_class_preds = xg_models.predict(X_test)"
      ],
      "metadata": {
        "id": "6Ex1el-4KvbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "xg_train_accuracy = accuracy_score(y_train_sm,xg_train_class_preds)\n",
        "xg_test_accuracy = accuracy_score(y_test,xg_test_class_preds)\n",
        "print(\"The accuracy on train dataset is\", xg_train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", xg_test_accuracy)"
      ],
      "metadata": {
        "id": "pzdWxNmwNqKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "labels = [ 'No Risk','Risk']\n",
        "xgb_cm = confusion_matrix(y_test,xg_test_class_preds)\n",
        "print(xgb_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(xgb_cm, annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test,xg_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "26J1cZAvOL2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "'''\n",
        "n_estimators = [50,80,100]\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xg_model = XGBClassifier()\n",
        "'''\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators = [50,80,100]\n",
        "# Maximum depth of trees\n",
        "max_depth = [8]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [30,40]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [20,30]\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "Z2NQhLV1LExh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search\n",
        "#(max_depth=8, min_samples_leaf=40, min_samples_split=50)\n",
        "#(max_depth=8, min_samples_leaf=20, min_samples_split=30)\n",
        "\n",
        "xg_grid = GridSearchCV(estimator=xg_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='roc_auc')\n",
        "\n",
        "xg_grid1=xg_grid.fit(X_train_sm,y_train_sm)\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "train_class_preds = xg_grid1.predict(X_train_sm)\n",
        "test_class_preds = xg_grid1.predict(X_test)\n"
      ],
      "metadata": {
        "id": "w472Vok_KEfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rm7Ds8UOH9aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xg_grid1.best_estimator_"
      ],
      "metadata": {
        "id": "P51-IBOuHu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_optimal_model =xg_grid1.best_estimator_"
      ],
      "metadata": {
        "id": "q1mRecR_IQaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_optimal_model.fit(X_train_sm, y_train_sm)"
      ],
      "metadata": {
        "id": "pPCrZvpQIgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "XGB_opt_train_class_preds = XGB_optimal_model.predict(X_train_sm)\n",
        "XGB_opt_test_class_preds = XGB_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "kSgHusmhIgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test\n",
        "XGB_opt_train_accuracy = accuracy_score(y_train_sm,XGB_opt_train_class_preds)\n",
        "XGB_opt_test_accuracy = accuracy_score(y_test,XGB_opt_test_class_preds)\n",
        "print(\"The accuracy on train dataset is\", XGB_opt_train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", XGB_opt_test_accuracy)"
      ],
      "metadata": {
        "id": "ebF_7sRJIgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [ 'No Risk','Risk']\n",
        "xgb_opt_cm = confusion_matrix(y_test,XGB_opt_test_class_preds)\n",
        "print(xgb_opt_cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(xgb_opt_cm, annot=True, ax = ax, fmt='g') #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "pin1BQZGIgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test,XGB_opt_test_class_preds,digits = 3))"
      ],
      "metadata": {
        "id": "hvvokcZuIgHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for data set before tuning:\n",
        "\n",
        "The accuracy on train dataset is 0.7809379070777247\n",
        "\n",
        "The accuracy on test dataset is 0.7786458333333334\n",
        "\n",
        "**confusion Matrix**\n",
        "\n",
        "               precision    recall  f1-score  \n",
        "           0      0.768     0.828     0.797      \n",
        "           1      0.792     0.724     0.757      \n",
        "\n",
        "for data set After tuning:\n",
        "\n",
        "The accuracy on train dataset is 0.8627876682587928\n",
        "\n",
        "The accuracy on test dataset is 0.8168402777777778\n",
        "\n",
        "**confusion Matrix**\n",
        "\n",
        "                precision    recall  f1-score   \n",
        "           0      0.822     0.831     0.826       \n",
        "           1      0.811     0.801     0.806       \n",
        "\n",
        "XGboost had done great improvment after tunning."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of eli5 for mode explainability."
      ],
      "metadata": {
        "id": "4MB3jEHReORU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install eli5"
      ],
      "metadata": {
        "id": "PeyCZ7I0VEzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5 as eli"
      ],
      "metadata": {
        "id": "d8VrY2o5VzLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eli5 for XGB model"
      ],
      "metadata": {
        "id": "DBg0BEgQeZrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eli.explain_weights(XGB_optimal_model)"
      ],
      "metadata": {
        "id": "O1cHC58kWDCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_importances = XGB_optimal_model.feature_importances_\n",
        "\n",
        "xgb_importance_dict = {'Feature' : list(Final_column.columns),\n",
        "                   'Feature Importance' : xgb_importances}\n",
        "\n",
        "xgb_importance_df = pd.DataFrame(xgb_importance_dict)"
      ],
      "metadata": {
        "id": "GB7_YwSYZGG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "uaJIvd6UZtS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eli5 for Random forest Model"
      ],
      "metadata": {
        "id": "ZwLky1F0eilo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eli.explain_weights(rf_optimal_model)"
      ],
      "metadata": {
        "id": "9JonRc63WbrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2VbomZWvlUQ"
      },
      "source": [
        "importances = rf_optimal_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(Final_column.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j973SktSwFHr"
      },
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tDependent variable i.e. TenYearCHD has little correlation between features and of all the variable only Age has strongest weight in predicting CHD , followed by blood pressure variable, glucose and  Prevalence of hypertension.\n",
        "\n",
        "•\tGender variable has large positive correlation in predicting dependent variable for XGBmodel,where as Age for RF model. Education had equal importance in both the model\n",
        "\n",
        "•\tAfter Hyper parameter tuning Logistic regression did not have much difference , but for Rf  train and test had very less difference in accuracy, which helped model in overfitting. \n",
        "\n",
        "•\tOver all XBG Xgboost had performed well after tunning\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}